{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Least Squares Regression.\n",
    "```\n",
    "- In this lecture and series of exercises you will learn:\n",
    "    - The concepts of variance and covariance of data.\n",
    "    - What the correlation coefficient is and how it relates to covariance.\n",
    "    - How linear regression relates to correlation.\n",
    "    - How to transform data with nonlinear relationship so that they may be analyzed with linear regression.\n",
    "    - The extension of linear regression to multiple dimensions.\n",
    "    - Application of the t-test to determine the significance of predictors in a multidimensional linear model.\n",
    "```\n",
    "#### What is your ability after finish this lecture.\n",
    "\n",
    " - Calculate the correlation coefficient for a 2D data set.\n",
    " - Find the least squares solution for a multi-variate linear regression problem.\n",
    " - Apply linear regression to nonlinear relationships through **variable transforms** .\n",
    " - Perform model selection on the predictors using a t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bofre start you need to install the following packages.\n",
    "\n",
    "```\n",
    "\n",
    "As part of this lecture, you will be applying all of the above concepts to analyze real data in the context of astronomy. To do this, you will need a data analysis environment set up.\n",
    "\n",
    "We recommend using Python, with the numpy and scipy packages. The data and solutions will be given in Python using these libraries. However, you are welcome to use any programming language or environment that you wish.\n",
    "\n",
    "You will also be asked to visualize the data. In Python, you can do this with the matplotlib library.\n",
    "\n",
    "For example, a scatter plot can be creating using\n",
    "```py\n",
    "import matplotlib.pyplot as plt # import the library\n",
    "plt.scatter(Xs, Ys) # Create the scatter plot, Xs and Ys are two numpy arrays of the same length\n",
    "plt.show() # Display the plot you just created.\n",
    "# A line plot can be created using\n",
    "\n",
    "plt.plot(Xs, Ys)\n",
    "# This will draw a line through the X, Y pairs defined by the Xs and Ys numpy arrays.\n",
    "\n",
    "# When working with matrices, numpy provides some convenient facilities. For example, to find the inverse of a matrix, use\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.inv(matrix_to_invert)\n",
    "# The scipy package provides statistical distributions. For example, to calculate p-values for the t-distribution, you can use the survival function (sf):\n",
    "\n",
    "import scipy.stats\n",
    "scipy.stats.t.sf(T, num_degrees_of_freedom)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation. --> `part 2`\n",
    "\n",
    "- what we nee to learn on this unit?\n",
    "    - Build up the concept of correlations from the more basic concepts of variance and covariance of data.\n",
    "    - Apply the concept of correlation to data from astronomy.\n",
    "\n",
    "- what is Covariance(التغاير)?\n",
    "  - Covariance is a measure of the amount of correlation between two variables.\n",
    "  - Conariance Eqution:\n",
    "    - $Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline X)(Y_i - \\overline Y)$\n",
    "    - $ \\overline X = \\frac{1}{n} \\sum_{i=1}^{n} X_i $  -> `mean of X`\n",
    "    - $ \\overline Y = \\frac{1}{n} \\sum_{i=1}^{n} Y_i $  -> `mean of Y`\n",
    "    - X_i is the ith value of X.\n",
    "    - Y_i is the ith value of Y.\n",
    "    - n is the number of values in X and Y.\n",
    "    - $Cov(X,Y)$ is the covariance of X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create the data\n",
    "X = np.random.normal(0, 1, 100)\n",
    "Y = np.random.normal(0, 1, 100)\n",
    "\n",
    "\n",
    "# Calculate the covariance\n",
    "Cov = np.cov(X, Y)[0, 1]\n",
    "\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "Corr = Cov / (np.std(X) * np.std(Y))\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Point about Covariance.\n",
    "- if the mean value  of $\\overline X = sum_{i=1}^{n} X_i / n$ and $\\overline Y = sum_{i=1}^{n} Y_i / n$, to cheange thats we will applying the `Bessel's correction`. \n",
    "\n",
    "    - $Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline X)(Y_i - \\overline Y)$.\n",
    "\n",
    "---\n",
    "- What does covariance demonstrate?\n",
    "    - Covariance measures the direction of the relationship between two variables. A positive covariance means that both variables tend to be high or low at the same time. A negative covariance means that when one variable is high, the other tends to be low.\n",
    "--- \n",
    "- What is covariance and variance?\n",
    "    - Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables.\n",
    "---\n",
    "- How do you interpret covariance in statistics?\n",
    "    \n",
    "    ![covarinance](./assets/img/covariance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "```text\n",
    "In this exercise, we will investigate the correlation present in astronomical data observed by Edwin Hubble in the period surrounding 1930.\n",
    "\n",
    "Hubble was interested in the motion of distant galaxies. He recorded the apparent velocity of these galaxies – the speed at which they appear to be receding away from us – by observing the spectrum of light they emit, and the distortion thereof caused by their relative motion to us. He also determined the distance of these galaxies from our own by observing a certain kind of star known as a Cepheid variable which periodically pulses. The amount of light this kind of star emits is related to this pulsation, and so the distance to any star of this type can be determined by how bright or dim it appears.\n",
    "\n",
    "The following figure shows his data. The Y-axis is the apparent velocity, measured in kilometers per second. Positive velocities are galaxies moving away from us, negative velocities are galaxies that are moving towards us. The Y-axis is the distance of the galaxy from us, measured in mega-parsecs (Mpc); one parsec is 3.26 light-years, or 30.9 trillion kilometers\n",
    "```\n",
    "\n",
    "![Hubble data](./assets/img/images_hubble_regression.png)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubble Dataset\n",
    "\n",
    "Xs = np.array([0.0339, 0.0423, 0.213, 0.257, 0.273, 0.273, 0.450, 0.503, 0.503, \\\n",
    "0.637, 0.805, 0.904, 0.904, 0.910, 0.910, 1.02, 1.11, 1.11, 1.41, \\\n",
    "1.72, 2.03, 2.02, 2.02, 2.02])\n",
    "\n",
    "Ys = np.array([-19.3, 30.4, 38.7, 5.52, -33.1, -77.3, 398.0, 406.0, 436.0, 320.0, 373.0, \\\n",
    "93.9, 210.0, 423.0, 594.0, 829.0, 718.0, 561.0, 608.0, 1.04E3, 1.10E3, \\\n",
    "840.0, 801.0, 519.0])\n",
    "\n",
    "N = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean for dataset.\n",
    "\n",
    "- why we need to calculate the mean of the dataset?\n",
    "    - to calculate the covariance.\n",
    "    - to calculate the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_X =  0.9199250000000001\n",
      "mu_Y =  425.6175\n"
     ]
    }
   ],
   "source": [
    "# mean for Xs and Ys\n",
    "mu_X = np.mean(Xs)  #Or Xs.sum()/N\n",
    "mu_Y = np.mean(Ys)  #Or Ys.sum()/N\n",
    "print(\"mu_X = \", mu_X)\n",
    "print(\"mu_Y = \", mu_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard deviation for dataset.\n",
    "\n",
    "- why we need to calculate the standard deviation of the dataset?\n",
    "    - to calculate the correlation coefficient.\n",
    "- how many equations we need to calculate the standard deviation of the dataset?\n",
    "    - 2 equations.\n",
    "- what is two equations?\n",
    "    - $ \\sigma_X = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline X)^2} $\n",
    "    - $ \\sigma_Y = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (Y_i - \\overline Y)^2} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_X =  0.6533948258734996\n",
      "sigma_Y =  348.7336574977229\n"
     ]
    }
   ],
   "source": [
    "# standard deviation for Xs and Ys\n",
    "sigma_X = np.sqrt(np.sum((Xs - mu_X)**2)/(N-1))\n",
    "sigma_Y = np.sqrt(np.sum((Ys - mu_Y)**2)/(N-1))\n",
    "print(\"sigma_X = \", sigma_X)\n",
    "print(\"sigma_Y = \", sigma_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let us now calculate the convariance of the dataset.\n",
    "\n",
    "we will use this equation:\n",
    "\n",
    "$ Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline X)(Y_i - \\overline Y)$\n",
    "\n",
    "- $\\sum_{i=1}^{n}(X_i - \\overline X)$.\n",
    "- $\\sum_{i=1}^{n}(Y_i - \\overline Y)$.\n",
    "- $n$ is the number of values in X and Y.\n",
    "- $n-1$ is the number of values in X and Y minus 1.\n",
    "- $X_i$ is the ith value of X.\n",
    "- $Y_i$ is the ith value of Y.\n",
    "- $\\overline X$ is the mean of X.\n",
    "- $\\overline Y$ is the mean of Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance = 191.20706528260868\n"
     ]
    }
   ],
   "source": [
    "# Now, calculate the sample covariance for the Hubble dataset to three significant figures:\n",
    "covariance = ((Xs - mu_X) * (Ys - mu_Y)).sum() / (N - 1)\n",
    "print(\"covariance =\", covariance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Correlation Coefficient --> `part 3`\n",
    "\n",
    "- what is differnt between correlation coefficient and covariance?\n",
    "    - correlation coefficient is a measure of the strength of the linear relationship between two variables.\n",
    "    - covariance is a measure of the amount of correlation between two variables.\n",
    "- if X and Y are positively correlated, the correlation coefficient is positive.\n",
    "- if X and Y are negatively correlated, the correlation coefficient is negative.\n",
    "- if X and Y are independent, the correlation coefficient is zero.\n",
    "- if X and Y are correlation coefficient is zero when X and Y are uncorrelated(remember that this dose mean that the two variables are independent).\n",
    "- Correlation don't take any units.\n",
    "    - exmaple on Hubble dataset correlation coefficient is 0.9 with out unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr = 0.8391399162310663\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation coefficient\n",
    "Corr = covariance / (sigma_X * sigma_Y)\n",
    "print(\"Corr =\", Corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratics.\n",
    "\n",
    "- Quadratics vs Linear vs Exponential.\n",
    "    - Quadratics are the simplest form of regression.\n",
    "    - Linear regression is the simplest form of regression.\n",
    "    - Exponential regression is the simplest form of regression.\n",
    "\n",
    "    ![Grap](./assets/img/linear-quadratic-exponential.jpg)\n",
    "\n",
    "\n",
    "- what is a quadratic?\n",
    "    - a quadratic is a function of the form $f(x) = ax^2 + bx + c$.\n",
    "        - $f(x)$ is the function.\n",
    "        - $a$ is the coefficient of the quadratic.\n",
    "            - $a = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\overline X)^2$\n",
    "        - $b$ is the coefficient of the linear term.\n",
    "            - $b = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline X)$\n",
    "        - $c$ is the constant term.\n",
    "            - $c = \\overline Y - \\overline X$\n",
    "- what is the quadratic formula?\n",
    "    - the quadratic formula is a way of finding the roots of a quadratic equation.\n",
    "    - the quadratic formula is $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$\n",
    "    - $b^2 - 4ac$ is the discriminant.\n",
    "    - $b$ is the coefficient of the linear term.\n",
    "    - $a$ is the coefficient of the quadratic term.\n",
    "    - $c$ is the constant term.\n",
    "___\n",
    "- Quadratic relation between X and Y.\n",
    "    - $y = a x^2 + b x + c$\n",
    "    - $a = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\overline X)^2$\n",
    "    - $b = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline X)$\n",
    "    - $c = \\overline Y - \\overline X$\n",
    "- suppose $Y =X^2$. what is the correlation between random variables X and Y ?\n",
    "\n",
    "```text\n",
    "A may seems very tempting. \"Correlation only measures variables' relations up to linear relations\" is conceptually correct, but we need to be careful with what it actually means. A seemingly purely nonlinear relation can have \"linear parts\", depending on how X is distributed. Let's first see two examples.\n",
    "```\n",
    "Exmaple One:\n",
    "\n",
    "$ X= 0$ with probability 0.5 and $X = 2$ otherwise.\n",
    "\n",
    "step 1:\n",
    "Than $Cov(X,Y) = E(XY) - E(X)E(Y) = 4 - (1 * 2) = 2$\n",
    "\n",
    "step2: \n",
    "$Correlation(X,Y) = Cov(X,Y) / (std(X) * std(Y)) = 2 / (1 * 2) = 1$\n",
    "Or \n",
    "$Correlation(X,Y) = Cov(X,Y) / \\sqrt{Var(X) Var(Y)} = 2 / \\sqrt{1 * 2} = 1$\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " real and different roots \n",
      "2.0\n",
      "-12.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def equationroots( a, b, c): \n",
    "  \n",
    "    # calculating discriminant using formula\n",
    "    dis = b * b - 4 * a * c \n",
    "    sqrt_val = math.sqrt(abs(dis)) \n",
    "      \n",
    "    # checking condition for discriminant\n",
    "    if dis > 0: \n",
    "        print(\" real and different roots \") \n",
    "        print((-b + sqrt_val)/(2 * a)) \n",
    "        print((-b - sqrt_val)/(2 * a)) \n",
    "      \n",
    "    elif dis == 0: \n",
    "        print(\" real and same roots\") \n",
    "        print(-b / (2 * a)) \n",
    "      \n",
    "    # when discriminant is less than 0\n",
    "    else:\n",
    "        print(\"Complex Roots\") \n",
    "        print(- b / (2 * a), \" + i\", sqrt_val) \n",
    "        print(- b / (2 * a), \" - i\", sqrt_val) \n",
    "  \n",
    "# Driver Program \n",
    "a = 1\n",
    "b = 10\n",
    "c = -24\n",
    "  \n",
    "# If a is 0, then incorrect equation\n",
    "if a == 0: \n",
    "        print(\"Input correct quadratic equation\") \n",
    "  \n",
    "else:\n",
    "    equationroots(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression `part 4`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to construct a linear predictive model `Part 4`\n",
    "- simple linear regression equation:\n",
    "    - $y = \\beta_0 + \\beta_1 x$.\n",
    "        - $y$ is the dependent variable.\n",
    "        - $x$ is the independent variable.\n",
    "        - $\\beta_0$ is the intercept.\n",
    "        - $\\beta_1$ is the slope.\n",
    "    - slop equation:\n",
    "        - $\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$.\n",
    "    - intercept equation:\n",
    "        - $\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness fit metric  (R-squared)\n",
    "is a measure of how well a regression model fits the data.\n",
    "\n",
    "$R^2$ = 1- $\\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "- $R^2$ is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. the independent variable.\n",
    "\n",
    "- $SS_{res}$ is the sum of the squared residuals.\n",
    "    - $SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "        -   $y_i$ is the actual value of the dependent variable.\n",
    "        -   $\\hat{y}_i$ is the predicted value of the dependent variable.\n",
    "\n",
    "- $SS_{tot}$ is the sum of the squared total.\n",
    "    - $SS_{tot} = \\sum_{i=1}^{n} (y_i - \\overline{y})^2$\n",
    "        - $\\overline{y}$ is the mean of the dependent variable.\n",
    "        - $y_i$ is the actual value of the dependent variable.\n",
    "\n",
    "#### What does R-squared tell us in regression?\n",
    "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.\n",
    "\n",
    "#### Is a higher R-squared better?\n",
    "In general, the higher the R-squared, the better the model fits your data.\n",
    "\n",
    "#### when use R-squared to evaluate a model?\n",
    "R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale. After fitting a linear regression model, you need to determine how well the model fits the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodness of fit metric (R-squared)\n",
    "def r_squared(xs, ys):\n",
    "    mean = np.mean(ys)\n",
    "    ss_tot = sum((ys - mean) ** 2)\n",
    "    ss_res = sum((ys - xs) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line reqgression model for the Hubble dataset\n",
    "def line_regression(xs, ys):\n",
    "    slope = covariance / (sigma_X * sigma_Y)\n",
    "    intercept = mu_Y - slope * mu_X\n",
    "    return slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope = 0.8391399162310663\n",
      "intercept = 424.8455542125611\n"
     ]
    }
   ],
   "source": [
    "slope , intercept = line_regression(Xs, Ys)\n",
    "print(\"slope =\", slope)\n",
    "print(\"intercept =\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some hubble data to test the line regression model\n",
    "Xs_test = np.array([0.0339, 0.0423, 0.213, 0.257, 0.273, 0.273, 0.450, 0.503, 0.503, \\\n",
    "0.637, 0.805, 0.904, 0.904, 0.910, 0.910, 1.02, 1.11, 1.11, 1.41, \\\n",
    "1.72, 2.03, 2.02, 2.02, 2.02])\n",
    "Ys_test = np.array([-19.3, 30.4, 38.7, 5.52, -33.1, -77.3, 398.0, 406.0, 436.0, 320.0, 373.0, \\\n",
    "93.9, 210.0, 423.0, 594.0, 829.0, 718.0, 561.0, 608.0, 1.04E3, 1.10E3, \\\n",
    "840.0, 801.0, 519.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ys_pred = [424.87400106 424.88104983 425.02429101 425.06121317 425.07463941\n",
      " 425.07463941 425.22316717 425.26764159 425.26764159 425.38008634\n",
      " 425.52106185 425.6041367  425.6041367  425.60917154 425.60917154\n",
      " 425.70147693 425.77699952 425.77699952 426.02874149 426.28887487\n",
      " 426.54900824 426.54061684 426.54061684 426.54061684]\n"
     ]
    }
   ],
   "source": [
    "# predict the Hubble data using the line regression model\n",
    "Ys_pred = intercept + slope * Xs_test\n",
    "print(\"Ys_pred =\", Ys_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_squared_residuals = 2789775.007231944\n"
     ]
    }
   ],
   "source": [
    "sum_of_squared_residuals = sum((Ys_test - Ys_pred) ** 2)\n",
    "print(\"sum_of_squared_residuals =\", sum_of_squared_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_squared_total = 2797148.76905\n"
     ]
    }
   ],
   "source": [
    "sum_of_squared_total = sum((Ys_test - np.mean(Ys_test)) ** 2)\n",
    "print(\"sum_of_squared_total =\", sum_of_squared_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.0026361707677637902\n"
     ]
    }
   ],
   "source": [
    "r2 = 1 - (sum_of_squared_residuals / sum_of_squared_total)\n",
    "print(\"r2 =\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##  Correcting simple nonlinear relationships `Part 5`\n",
    "\n",
    "### Understanding Linear Relationships.\n",
    "A linear relationship exists when two quantities are proportional to each other. If you increase one of the quantities, the other quantity either increases or decreases at a constant rate. For example, if you get paid $10 an hour, there is a linear relationship between your hours worked and your pay. Working another hour always results in a $10 pay increase, regardless of how many hours you already worked.\n",
    "\n",
    "nonlinear relationships are monotonic, meaning they always increase or decrease, but not both. Monotonic relationships differ from linear relationships because they do not increase or decrease at a constant rate. When graphed, they appear as curves. If a monotonic relationship occurs where increases in one entity cause a decrease in the other entity, this is called an inverse relationship. However, nonlinear relationships can also be too irregular to fit any of these categories.\n",
    "\n",
    "### Understanding Nonlinear Relationships\n",
    "Nonlinear relationships, and often monotonic relationships, arise regularly when comparing geometrical measurements of a single shape. For example, there is a monotonic nonlinear relationship between the radius of a sphere and the volume of that same sphere. Nonlinear relationships also appear in real world situations, such as in the relationship between the value of a motorcycle and the amount of time you owned the motorcycle, or in the amount of time it takes to do a job in relation to the number of people there to help. If your boss raises your hourly rate to $15 per hour when you work overtime, the relationship of your hours worked to your pay acquired might become nonlinear.\n",
    "\n",
    "### 5 Examples of Nonlinear Relationships Between Variables.\n",
    "[link](https://www.statology.org/nonlinear-relationship-examples/)\n",
    "___\n",
    "#### Example 1: Quadratic Relationships\n",
    "One of the most common nonlinear relationships in the real world is a quadratic relationship between variables.\n",
    "\n",
    "When plotted on a scatterplot, this relationship typically exhibits a “U” shape.\n",
    "\n",
    "One example might be total working hours per week vs. overall happiness:\n",
    "\n",
    "![Exmaple1](./assets/img/nonlinear-exmaple1.jpg)\n",
    "\n",
    "As working hours increase from zero, overall happiness tends to increase, but beyond a certain threshold more working hours actually leads to decreased happiness.\n",
    "\n",
    "This upside down “U” shape is the signature shape of a quadratic relationship between two variables.\n",
    "___\n",
    "\n",
    "#### Example 3: Exponential Relationships\n",
    "\n",
    "Another common nonlinear relationship in the real world is a cubic relationship between variables.\n",
    "\n",
    "When plotted on a scatterplot, this relationship typically has two distinct curves.\n",
    "\n",
    "This type of relationship exists often between variables in the field of thermodynamics:\n",
    "\n",
    "![exmapl2](./assets/img/nonlinear2.jpg)\n",
    "\n",
    "During the first few years of growth, a bamboo plant grows very slowly but once it reaches a certain age it explodes in height and grows at a rapid pace.\n",
    "___\n",
    "\n",
    "#### Example 4: Logarithmic Relationships\n",
    "Another common nonlinear relationship in the real world is a logarithmic relationship between variables.\n",
    "\n",
    "When plotted on a scatterplot, this relationship exhibits a single curve that becomes less pronounced as the variable on the x-axis increases.\n",
    "\n",
    "One example of a logarithmic relationship is between the efficiency of smart-home technologies and time:\n",
    "![exmaple3](./assets/img/nonlinear3.jpg)\n",
    "\n",
    "When a new smart-home technology (like a self-operating vacuum or self-operating AC unit) is installed in a home, it learns rapidly how to become more efficient, but then once it reaches a certain point it hits a maximum threshold in efficiency.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array([ 0.387, 0.723, 1.00, 1.52, 5.20, 9.54, 19.2, 30.1, 39.5 ])\n",
    "\n",
    "Ys = np.array([ 0.241, 0.615, 1.00, 1.88, 11.9, 29.5, 84.0, 165.0, 248 ])\n",
    "\n",
    "N = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0.1529117929472797\n",
      "w = 45465357.95424496\n",
      "Y = [17.48729739 17.58286489 17.63246123 17.69648698 17.88456058 17.97735278\n",
      " 18.08430186 18.15305328 18.19461096]\n"
     ]
    }
   ],
   "source": [
    "# log(YS)= k*log(XS) + log(w)\n",
    "k = (np.log(Ys) - np.log(Xs)).sum() / (np.log(Xs) ** 2).sum()\n",
    "w = np.exp(np.log(Ys).sum() - k * np.log(Xs).sum())\n",
    "Y = k * np.log(Xs) + np.log(w)\n",
    "\n",
    "print(\"k =\", k)\n",
    "print(\"w =\", w)\n",
    "print(\"Y =\", Y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
